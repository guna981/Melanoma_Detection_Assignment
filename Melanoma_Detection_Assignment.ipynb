{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_DqC0PIORD2"
      },
      "source": [
        "**Problem statement**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asbF36xTPLrJ"
      },
      "source": [
        "To build a CNN based model which can accurately detect melanoma. Melanoma is a type of cancer that can be deadly if not detected early. It accounts for 75% of skin cancer deaths. A solution that can evaluate images and alert dermatologists about the presence of melanoma has the potential to reduce a lot of manual effort needed in diagnosis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IXo9gZuO7B4"
      },
      "outputs": [],
      "source": [
        "#importing libraries needed\n",
        "import pathlib\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import PIL\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from glob import glob\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "#To avoid any warnings in the middle importing warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_whmFChHApx"
      },
      "source": [
        "**Connecting to google drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tzdWsdggg6c"
      },
      "outputs": [],
      "source": [
        "#mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbbEYPuBgg93"
      },
      "outputs": [],
      "source": [
        "#unzip the dataset\n",
        "!unzip \"/content/drive/MyDrive/CNN_assignment.zip\" > /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6gpX_SuIFXz"
      },
      "source": [
        "A dataset contains 2357 images of skin cancer types. The dataset contains 9 sub-directories in each train and test subdirectories. The 9 sub-directories contains the images of 9 skin cancer types respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdPPsakLIP8O"
      },
      "source": [
        "**Defining the path for train and test images** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31o9MDFug_Ws"
      },
      "outputs": [],
      "source": [
        "# Defining the path for train and test images\n",
        "data_dir_train =pathlib.Path(\"/content/Skin cancer ISIC The International Skin Imaging Collaboration/Train/\")\n",
        "data_dir_test = pathlib.Path(\"/content/Skin cancer ISIC The International Skin Imaging Collaboration/Test/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHG0FoICk7He"
      },
      "outputs": [],
      "source": [
        "# Count the number of image in Train and Test directory\n",
        "# Using the glob to retrieve files/pathnames matching a specified pattern.\n",
        "\n",
        "#Train Image count\n",
        "image_count_train = len(list(data_dir_train.glob('*/*.jpg')))\n",
        "print(image_count_train)\n",
        "\n",
        "#Test Image count\n",
        "image_count_test = len(list(data_dir_test.glob('*/*.jpg')))\n",
        "print(image_count_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KwKOoMsLe5f"
      },
      "source": [
        "**Create a Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_KSD2HGNCDm"
      },
      "source": [
        "Define some parameters for the loader:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xl4ai1yINKtE"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "img_height = 180\n",
        "img_width = 180"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eO8f2avOHpO"
      },
      "source": [
        "Using 80% of the images for training, and 20% for validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9HHlsfBNMQj"
      },
      "outputs": [],
      "source": [
        "#Train datset\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(data_dir_train,batch_size=batch_size,image_size=(img_height,img_width),label_mode='categorical',\n",
        "                                                              seed=123,subset=\"training\",validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twtv7czqOYB2"
      },
      "outputs": [],
      "source": [
        "#Validation Dataset\n",
        "val_ds =tf.keras.preprocessing.image_dataset_from_directory(data_dir_train,batch_size=batch_size,image_size=(img_height,img_width),label_mode='categorical',\n",
        "                                                              seed=123,subset=\"validation\",validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwGtTkc4Oc9g"
      },
      "outputs": [],
      "source": [
        "# List out all the classes of skin cancer and store them in a list. \n",
        "# You can find the class names in the class_names attribute on these datasets. \n",
        "# These correspond to the directory names in alphabetical order.\n",
        "class_names = train_ds.class_names\n",
        "print(class_names)\n",
        "     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlJcPH1-hnXC"
      },
      "source": [
        "**Data Visulization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npI-xsOwk7J9"
      },
      "outputs": [],
      "source": [
        "#Dictionary to store the path of image as per the class\n",
        "files_path_dict = {}\n",
        "\n",
        "for c in class_names:\n",
        "    files_path_dict[c] = list(map(lambda x:str(data_dir_train)+'/'+c+'/'+x,os.listdir(str(data_dir_train)+'/'+c)))\n",
        "    \n",
        "#Visualize image \n",
        "plt.figure(figsize=(15,15))\n",
        "index = 0\n",
        "for c in class_names:\n",
        "    path_list = files_path_dict[c][:1]\n",
        "    index += 1\n",
        "    plt.subplot(3,3,index)\n",
        "    plt.imshow(load_img(path_list[0],target_size=(img_height,img_width)))\n",
        "    plt.title(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BN1Q5vi8jHD-"
      },
      "source": [
        "The image_batch is a tensor of the shape (32, 180, 180, 3). This is a batch of 32 images of shape 180x180x3 (the last dimension refers to color channels RGB). The label_batch is a tensor of the shape (32,), these are corresponding labels to the 32 images.\n",
        "\n",
        "Dataset.cache() keeps the images in memory after they're loaded off disk during the first epoch.\n",
        "\n",
        "Dataset.prefetch() overlaps data preprocessing and model execution while training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwFtwWoeNJBk"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAAKDyvEjWL5"
      },
      "source": [
        "**Create the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsNgAisNlRPv"
      },
      "outputs": [],
      "source": [
        "input_shape = (img_height,img_width,3)\n",
        "\n",
        "model = Sequential()    #Sequential allows you to create models layer-by-layer  \n",
        "\n",
        "#First Convulation Layer\n",
        "model.add(layers.experimental.preprocessing.Rescaling(1./255,input_shape=input_shape))\n",
        "model.add(layers.Conv2D(32,kernel_size=(3,3),activation='relu'))\n",
        "model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
        "\n",
        "#Second Convulation Layer\n",
        "model.add(layers.Conv2D(64,kernel_size=(3,3),activation='relu'))\n",
        "model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
        "\n",
        "#Third Convulation Layer\n",
        "model.add(layers.Conv2D(128,kernel_size=(3,3),activation='relu'))\n",
        "model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(layers.Flatten())   #Keras.layers.flatten function flattens the multi-dimensional input tensors into a single dimension.\n",
        "\n",
        "#Dense Layer\n",
        "model.add(layers.Dense(512,activation='relu'))\n",
        "\n",
        "#Dense Layer\n",
        "model.add(layers.Dense(128,activation='relu'))\n",
        "\n",
        "#Dense Layer with softmax activation function.\n",
        "#Softmax is an activation function that scales numbers/logits into probabilities.\n",
        "model.add(layers.Dense(len(class_names),activation='softmax'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbiJryrVjxkV"
      },
      "source": [
        "**Compile the model** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNP75XQflRT7"
      },
      "outputs": [],
      "source": [
        "#Using Adam optimization technique\n",
        "#categorical_crossentropy: Used as a loss function for multi-class classification model where there are two or more output labels.\n",
        "\n",
        "model.compile(optimizer='Adam',\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-LmvCU1k7Nh"
      },
      "outputs": [],
      "source": [
        "# Getting summary of all layers\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zl-Yecc1kvtG"
      },
      "source": [
        "**Train the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRtQiPUok7QQ"
      },
      "outputs": [],
      "source": [
        "#Using 20 epochs for model1\n",
        "epochs = 20\n",
        "history = model.fit(\n",
        "  train_ds,\n",
        "  validation_data=val_ds,\n",
        "  epochs=epochs\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgNFG0e7mCO_"
      },
      "source": [
        "**Visualizing training results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kermrE2k8Ge"
      },
      "outputs": [],
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPmVlAEcmeFm"
      },
      "source": [
        "Model is overfitting. From the above Training accuarcy vs Validation accuracy graph we can see that there is large difference between training and validation accuracy as the epoch increases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D71XzJ4ck8Jn"
      },
      "outputs": [],
      "source": [
        "#Data augumentation\n",
        " #To rescale an input in the [0, 255] range to be in the [0, 1] range  \n",
        "rescale = tf.keras.Sequential([\n",
        " layers.experimental.preprocessing.Rescaling(1./255)\n",
        "])\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "  #Randomly flip each images\n",
        "  layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
        "    \n",
        "  #Randomly zoom each images during training\n",
        "  layers.experimental.preprocessing.RandomZoom(0.2),\n",
        "\n",
        "  #Randomly rotate each images\n",
        "  layers.experimental.preprocessing.RandomRotation(0.2),\n",
        "       \n",
        "  #Randomly translate each images during training\n",
        "  layers.experimental.preprocessing.RandomTranslation(0.1, 0.1),\n",
        "  #Randomly contrast each images during training\n",
        "  layers.experimental.preprocessing.RandomContrast(0.2)\n",
        "])\n",
        "\n",
        "#Visualize the augmentation image\n",
        "plt.figure(figsize=(10, 10))\n",
        "for images, _ in train_ds.take(1):   \n",
        "    for i in range(9):\n",
        "        augmented_images = data_augmentation(images)\n",
        "        \n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
        "        plt.axis(\"off\")    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g48rDcaoH2WX"
      },
      "source": [
        "**Model 2 Creation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NU14MsX30w7x"
      },
      "outputs": [],
      "source": [
        "#Dropout layer: randomly sets input units to 0 with a frequency of rate at each step during training time,\n",
        "#which helps prevent overfitting.Inputs not set to 0 are scaled up by 1/(1 - rate) such that the sum over all inputs is unchanged.\n",
        "\n",
        "\n",
        "## Your code goes here\n",
        "model2 = Sequential()                     #Sequential allows you to create models layer-by-layer  \n",
        "\n",
        "model2.add(data_augmentation)             #Augmentation layer\n",
        "model2.add(rescale)                       #Rescaling layer\n",
        "\n",
        "#First Convulation Layer\n",
        "model2.add(layers.Conv2D(32,kernel_size=(3,3),activation='relu'))\n",
        "model2.add(layers.MaxPool2D(pool_size=(2,2)))\n",
        "\n",
        "#Dropout layer with 25% Fraction of the input units to drop.\n",
        "model2.add(layers.Dropout(0.25))\n",
        "\n",
        "#Second Convulation Layer\n",
        "model2.add(layers.Conv2D(64,kernel_size=(3,3),activation='relu'))\n",
        "model2.add(layers.MaxPool2D(pool_size=(2,2)))\n",
        "\n",
        "#Dropout layer with 25% Fraction of the input units to drop.\n",
        "model2.add(layers.Dropout(0.25))\n",
        "\n",
        "#Third Convulation Layer\n",
        "model2.add(layers.Conv2D(128,kernel_size=(3,3),activation='relu'))\n",
        "model2.add(layers.MaxPool2D(pool_size=(2,2)))\n",
        "\n",
        "#Keras.layers.flatten function flattens the multi-dimensional input tensors into a single dimension.\n",
        "model2.add(layers.Flatten())\n",
        "\n",
        "#Dense Layer\n",
        "model2.add(layers.Dense(512,activation='relu'))\n",
        "\n",
        "#Dense Layer\n",
        "model2.add(layers.Dense(128,activation='relu'))\n",
        "\n",
        "#Dropout layer with 50% Fraction of the input units to drop.\n",
        "model2.add(layers.Dropout(0.50))\n",
        "\n",
        "#Dense Layer with softmax activation function.\n",
        "#Softmax is an activation function that scales numbers/logits into probabilities.\n",
        "model2.add(layers.Dense(len(class_names),activation='softmax'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZ7OHyI9Jn02"
      },
      "source": [
        "**Compiling the model2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lokZfF0q0w-x"
      },
      "outputs": [],
      "source": [
        "model2.compile(optimizer='Adam',\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_j4pYoxJvxb"
      },
      "source": [
        "**Training the model2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FeDhPSvP0xCm"
      },
      "outputs": [],
      "source": [
        "#Using 20 epochs for model2\n",
        "epochs =20\n",
        "history = model2.fit(train_ds,epochs=epochs,validation_data=val_ds,verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yHnZYkFKKbS"
      },
      "source": [
        "**Visualizing the results model2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfilF0elJ2-D"
      },
      "outputs": [],
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpR3LvsdU_wG"
      },
      "source": [
        "After performing data augumentation overfitting issue reduced.\n",
        "\n",
        "Let's check is there any class imbalance is there"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVvMx7l2Ca0N"
      },
      "source": [
        "**Distribution of classes in the training dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dql7F5I0KORy"
      },
      "outputs": [],
      "source": [
        "#count number of image in each classes\n",
        "def class_distribution_count(dir_train):\n",
        "       \n",
        "    count= []\n",
        "    for path in pathlib.Path(dir_train).iterdir():\n",
        "        if path.is_dir():\n",
        "            count.append(len([name for name in os.listdir(path)\n",
        "                               if os.path.isfile(os.path.join(path, name))]))\n",
        "    \n",
        "    #name of the classes\n",
        "    sub_dir = [name for name in os.listdir(dir_train)\n",
        "                    if os.path.isdir(os.path.join(dir_train, name))]\n",
        "    \n",
        "    #return dataframe with image count and class.\n",
        "    return pd.DataFrame(list(zip(sub_dir,count)),columns =['Class', 'Number of Image'])\n",
        "\n",
        "image_df = class_distribution_count(data_dir_train)\n",
        "image_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Poi-hfmDifc"
      },
      "outputs": [],
      "source": [
        "#Visualize the Number of image in each class.\n",
        "import seaborn as sns\n",
        "plt.figure(figsize=(10, 10))\n",
        "sns.barplot(x=\"Number of Image\", y=\"Class\", data=image_df,\n",
        "            label=\"Class\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bstyb-qsE-Um"
      },
      "source": [
        "pigmented benign keratosis(462 Samples) and melanoma (438 Samples) class has highest number of samples.\n",
        "\n",
        "Seborrheic keratosis(77 Samples) has the least number of samples.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuWg9f0LIREq"
      },
      "source": [
        "**Rectify the class imbalance**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-0LDh2yIX7F"
      },
      "source": [
        "**Context:** You can use a python package known as Augmentor (https://augmentor.readthedocs.io/en/master/) to add more samples across all classes so that none of the classes have very few samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Ue1vRrNDikG"
      },
      "outputs": [],
      "source": [
        "!pip install Augmentor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3-190NgIoJ0"
      },
      "source": [
        "To use Augmentor, the following general procedure is followed:\n",
        "\n",
        "Instantiate a Pipeline object pointing to a directory containing your initial image data set.\n",
        "Define a number of operations to perform on this data set using your Pipeline object.\n",
        "Execute these operations by calling the Pipelineâ€™s sample() method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdT9iRD7Ii80"
      },
      "outputs": [],
      "source": [
        "path_to_training_dataset=\"/content/Skin cancer ISIC The International Skin Imaging Collaboration/Train/\"\n",
        "import Augmentor\n",
        "for i in class_names:\n",
        "    p = Augmentor.Pipeline(path_to_training_dataset + i)\n",
        "    p.rotate(probability=0.7, max_left_rotation=10, max_right_rotation=10)\n",
        "    p.sample(500) ## We are adding 500 samples per class to make sure that none of the classes are sparse."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39orA6pTI6i8"
      },
      "source": [
        "Augmentor has stored the augmented images in the output sub-directory of each of the sub-directories of skin cancer types.. Lets take a look at total count of augmented images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXh9S3TRItu-"
      },
      "outputs": [],
      "source": [
        "#Count total number of image generated by using Augmentor\n",
        "image_count_train = len(list(data_dir_train.glob('*/output/*.jpg')))\n",
        "print(image_count_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNv9uvtjJG_W"
      },
      "source": [
        "**Lets see the distribution of augmented data after adding new images to the original training data.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSApbNAFItyW"
      },
      "outputs": [],
      "source": [
        "path_list = [x for x in glob(os.path.join(data_dir_train, '*','output', '*.jpg'))]\n",
        "path_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfiV9HRsJSfM"
      },
      "outputs": [],
      "source": [
        "\n",
        "lesion_list_new = [os.path.basename(os.path.dirname(os.path.dirname(y))) for y in glob(os.path.join(data_dir_train, '*','output', '*.jpg'))]\n",
        "lesion_list_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7QEnCPHJtTx"
      },
      "outputs": [],
      "source": [
        "dataframe_dict_new = dict(zip(path_list, lesion_list_new))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKZeSuz6JwyZ"
      },
      "outputs": [],
      "source": [
        "#dataframe that store path and label of the images generated by Augmentor\n",
        "df2 = pd.DataFrame(list(dataframe_dict_new.items()),columns = ['Path','Label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnN_1b8iJy1_"
      },
      "outputs": [],
      "source": [
        "#label count.\n",
        "df2['Label'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDbqHU_FKT3S"
      },
      "source": [
        "So, now we have added 500 images to all the classes to maintain some class balance. We can add more images as we want to improve training process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImWjZXk5KaBp"
      },
      "source": [
        "**Train the model on the data created using Augmentor**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdnFD-psKHEW"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "img_height = 180\n",
        "img_width = 180"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOwda9uYKhOD"
      },
      "source": [
        "**Create a training dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ki-WcNZbKHLr"
      },
      "outputs": [],
      "source": [
        "data_dir_train=\"/content/Skin cancer ISIC The International Skin Imaging Collaboration/Train/\" \n",
        "\n",
        "#Training dataset.\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  data_dir_train,\n",
        "  seed=123,\n",
        "  validation_split = 0.2,    #20% fraction of data to reserve for validation.\n",
        "  subset = \"training\",\n",
        "  image_size=(img_height, img_width),label_mode='categorical',  #label_mode='categorical' means that the labels are encoded as a categorical vector \n",
        "  batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uABPbZJXKw09"
      },
      "source": [
        "**Create a validation dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXHJ3zVlJtd-"
      },
      "outputs": [],
      "source": [
        "#Validation dataset.\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  data_dir_train,\n",
        "  seed=123,\n",
        "  validation_split = 0.2,\n",
        "  subset = \"validation\",\n",
        "  image_size=(img_height, img_width),label_mode='categorical',   #label_mode='categorical' means that the labels are encoded as a categorical vector\n",
        "  batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiPRxyUtK7mD"
      },
      "source": [
        "**Create model3**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbX73Pt9K0B3"
      },
      "outputs": [],
      "source": [
        "model3 = Sequential()\n",
        "\n",
        "model3.add(rescale)   #Rescaling Layer\n",
        "\n",
        "#First Convulation layer\n",
        "model3.add(layers.Conv2D(32,kernel_size=(2,2),activation='relu'))\n",
        "model3.add(layers.MaxPool2D(pool_size=(2,2)))\n",
        "model3.add(layers.Dropout(0.25))\n",
        "\n",
        "#Second Convulation Layer\n",
        "model3.add(layers.Conv2D(64,kernel_size=(2,2),activation='relu'))\n",
        "model3.add(layers.MaxPool2D(pool_size=(2,2)))\n",
        "model3.add(layers.Dropout(0.25))\n",
        "\n",
        "#Third Convulation Layer\n",
        "model3.add(layers.Conv2D(128,kernel_size=(2,2),activation='relu'))\n",
        "model3.add(layers.MaxPool2D(pool_size=(2,2)))\n",
        "\n",
        "#Flatten Layer\n",
        "model3.add(layers.Flatten())\n",
        "\n",
        "#Dense Layer\n",
        "model3.add(layers.Dense(512,activation='relu'))\n",
        "\n",
        "#Dropout layer\n",
        "model3.add(layers.Dropout(0.25))\n",
        "\n",
        "#Batch normalization helps to make artificial neural networks faster and more stable through normalization \n",
        "#of the layers' inputs by re-centering and re-scaling.\n",
        "model3.add(layers.BatchNormalization())\n",
        "\n",
        "#Dense Layer\n",
        "model3.add(layers.Dense(128,activation='relu'))\n",
        "\n",
        "#Dropout layer with 50% Fraction of the input units to drop.\n",
        "model3.add(layers.Dropout(0.50))\n",
        "\n",
        "#Batch normalization\n",
        "model3.add(layers.BatchNormalization())\n",
        "\n",
        "#Dense layer with Softmax activation function.\n",
        "model3.add(layers.Dense(len(class_names),activation='softmax'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R89JYWY5LQaq"
      },
      "source": [
        "**Compile Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXG4G8WnLOfP"
      },
      "outputs": [],
      "source": [
        "model3.compile(optimizer='Adam',\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qOVn2opLVbb"
      },
      "source": [
        "**Train model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZeuuvjGsLcVg"
      },
      "outputs": [],
      "source": [
        "#Using 30 epochs for model 3\n",
        "epochs = 30\n",
        "history = model3.fit(train_ds,epochs=epochs,validation_data=val_ds,verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nP8GpoNLk7W"
      },
      "source": [
        "**Visualize the model results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sA5vFLjFLiVo"
      },
      "outputs": [],
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKmXSQMiInSW"
      },
      "source": [
        "**Observations:**\n",
        "\n",
        "As per the final model Training accuracy and validation accuracy increases.\n",
        "\n",
        "Model overfitting issue is resolved.\n",
        "\n",
        "Class rebalance helps in achieving the best Training and validation accuracy and augmentation."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}